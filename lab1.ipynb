{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab1 - Scikit-learn\n",
    "Author: Michael Le\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "The goal of this lab is to become familiar with the scikit-learn library.\n",
    "\n",
    "You will practice loading example datasets, perform classification and regression with linear scikit-learn models, and investigate the effects of reducing the number of features (columns in X) and the number of samples (rows in X and y).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import yellowbrick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Classification\n",
    "\n",
    "Using yellowbrick spam - classification  \n",
    "https://www.scikit-yb.org/en/latest/api/datasets/spam.html\n",
    "\n",
    "The goal is to investigate `LogisticRegression(max_iter=2000)` and effects of reducing the number of features and number of samples on classification performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Implement convenience function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def get_classifier_accuracy(model, X, y):\n",
    "    '''Calculate train and validation accuracy of classifier (model)\n",
    "        \n",
    "        Splits feature matrix X and target vector y \n",
    "        with sklearn train_test_split() and random_state=956.\n",
    "        \n",
    "        model (sklearn classifier): Classifier to train and evaluate\n",
    "        X (numpy.array or pandas.DataFrame): Feature matrix\n",
    "        y (numpy.array or pandas.Series): Target vector\n",
    "        \n",
    "        returns: training accuracy, validation accuracy\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #TODO: IMPLEMENT FUNCTION BODY\n",
    "    # Split arrays or matrices into random train and test subsets. Use default 75/25 split.\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=956)\n",
    "    \n",
    "    # Fit the model according to the given training vector X_train and its relative target vector y_train\n",
    "    # Predict class labels for samples in X_test - X_test is the data matrix for which we want to get predictions\n",
    "    # Returns vector containing the class labels for each sample in X_test\n",
    "    model.fit(X_train, y_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Predict class labels for samples in X_train - X_train is the data matrix for which we want to get predictions\n",
    "    # Returns vector containing the class labels for each sample in X_train\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    \n",
    "    # Use the true labels and predicted labels(as returned by classifier) for the validation(testing) set and \n",
    "    # training set to determine accuracy classifcation score\n",
    "    validation_accuracy = accuracy_score(y_test, y_test_pred) # Fraction of correctly classified samples for test set\n",
    "    training_accuracy = accuracy_score(y_train, y_train_pred) # Fraction of correctly classified samples for training set\n",
    "    \n",
    "    return((training_accuracy, validation_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Load data\n",
    "\n",
    "Use the yellowbrick function `load_spam()`, load the spam data set into feature matrix `X` and target vector `y`.\n",
    "\n",
    "Print size and type of `X` and `y`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (4600, 57)\n",
      "Shape of y: (4600,)\n",
      "\n",
      "Type of X Columns: \n",
      "word_freq_make                float64\n",
      "word_freq_address             float64\n",
      "word_freq_all                 float64\n",
      "word_freq_3d                  float64\n",
      "word_freq_our                 float64\n",
      "word_freq_over                float64\n",
      "word_freq_remove              float64\n",
      "word_freq_internet            float64\n",
      "word_freq_order               float64\n",
      "word_freq_mail                float64\n",
      "word_freq_receive             float64\n",
      "word_freq_will                float64\n",
      "word_freq_people              float64\n",
      "word_freq_report              float64\n",
      "word_freq_addresses           float64\n",
      "word_freq_free                float64\n",
      "word_freq_business            float64\n",
      "word_freq_email               float64\n",
      "word_freq_you                 float64\n",
      "word_freq_credit              float64\n",
      "word_freq_your                float64\n",
      "word_freq_font                float64\n",
      "word_freq_000                 float64\n",
      "word_freq_money               float64\n",
      "word_freq_hp                  float64\n",
      "word_freq_hpl                 float64\n",
      "word_freq_george              float64\n",
      "word_freq_650                 float64\n",
      "word_freq_lab                 float64\n",
      "word_freq_labs                float64\n",
      "word_freq_telnet              float64\n",
      "word_freq_857                 float64\n",
      "word_freq_data                float64\n",
      "word_freq_415                 float64\n",
      "word_freq_85                  float64\n",
      "word_freq_technology          float64\n",
      "word_freq_1999                float64\n",
      "word_freq_parts               float64\n",
      "word_freq_pm                  float64\n",
      "word_freq_direct              float64\n",
      "word_freq_cs                  float64\n",
      "word_freq_meeting             float64\n",
      "word_freq_original            float64\n",
      "word_freq_project             float64\n",
      "word_freq_re                  float64\n",
      "word_freq_edu                 float64\n",
      "word_freq_table               float64\n",
      "word_freq_conference          float64\n",
      "char_freq_;                   float64\n",
      "char_freq_(                   float64\n",
      "char_freq_[                   float64\n",
      "char_freq_!                   float64\n",
      "char_freq_$                   float64\n",
      "char_freq_#                   float64\n",
      "capital_run_length_average    float64\n",
      "capital_run_length_longest      int64\n",
      "capital_run_length_total        int64\n",
      "dtype: object\n",
      "\n",
      "Type of X: \n",
      "object\n",
      "\n",
      "Type of y: \n",
      "int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: ADD YOUR CODE HERE\n",
    "from yellowbrick.datasets.loaders import load_spam\n",
    "\n",
    "# Load spam data set into feature matrix X and target vector y\n",
    "X, y = load_spam()\n",
    "# X.head()\n",
    "# y.head()\n",
    "print(f\"Shape of X: {X.shape}\")\n",
    "print(f\"Shape of y: {y.shape}\\n\")\n",
    "print(f\"Type of X Columns: \\n{X.dtypes}\\n\")\n",
    "print(f\"Type of X: \\n{X.dtypes.dtype}\\n\")\n",
    "print(f\"Type of y: \\n{y.dtypes}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_freq_make                0\n",
      "word_freq_address             0\n",
      "word_freq_all                 0\n",
      "word_freq_3d                  0\n",
      "word_freq_our                 0\n",
      "word_freq_over                0\n",
      "word_freq_remove              0\n",
      "word_freq_internet            0\n",
      "word_freq_order               0\n",
      "word_freq_mail                0\n",
      "word_freq_receive             0\n",
      "word_freq_will                0\n",
      "word_freq_people              0\n",
      "word_freq_report              0\n",
      "word_freq_addresses           0\n",
      "word_freq_free                0\n",
      "word_freq_business            0\n",
      "word_freq_email               0\n",
      "word_freq_you                 0\n",
      "word_freq_credit              0\n",
      "word_freq_your                0\n",
      "word_freq_font                0\n",
      "word_freq_000                 0\n",
      "word_freq_money               0\n",
      "word_freq_hp                  0\n",
      "word_freq_hpl                 0\n",
      "word_freq_george              0\n",
      "word_freq_650                 0\n",
      "word_freq_lab                 0\n",
      "word_freq_labs                0\n",
      "word_freq_telnet              0\n",
      "word_freq_857                 0\n",
      "word_freq_data                0\n",
      "word_freq_415                 0\n",
      "word_freq_85                  0\n",
      "word_freq_technology          0\n",
      "word_freq_1999                0\n",
      "word_freq_parts               0\n",
      "word_freq_pm                  0\n",
      "word_freq_direct              0\n",
      "word_freq_cs                  0\n",
      "word_freq_meeting             0\n",
      "word_freq_original            0\n",
      "word_freq_project             0\n",
      "word_freq_re                  0\n",
      "word_freq_edu                 0\n",
      "word_freq_table               0\n",
      "word_freq_conference          0\n",
      "char_freq_;                   0\n",
      "char_freq_(                   0\n",
      "char_freq_[                   0\n",
      "char_freq_!                   0\n",
      "char_freq_$                   0\n",
      "char_freq_#                   0\n",
      "capital_run_length_average    0\n",
      "capital_run_length_longest    0\n",
      "capital_run_length_total      0\n",
      "dtype: int64\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(X.isnull().sum()) # Check for null values in columns of X\n",
    "print(y.isnull().sum()) # Check for null values in columns of y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the sklearn function `train_test_split()` prepare a feature matrix `X_small` and target vector `y_small` that contain only **1%** of the rows. Use `random_state=174`.\n",
    "\n",
    "Print size and type of `X_small` and `y_small`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_small: (46, 57)\n",
      "Shape of y_small: (46,)\n",
      "\n",
      "Type of X_small Columns: \n",
      "word_freq_make                float64\n",
      "word_freq_address             float64\n",
      "word_freq_all                 float64\n",
      "word_freq_3d                  float64\n",
      "word_freq_our                 float64\n",
      "word_freq_over                float64\n",
      "word_freq_remove              float64\n",
      "word_freq_internet            float64\n",
      "word_freq_order               float64\n",
      "word_freq_mail                float64\n",
      "word_freq_receive             float64\n",
      "word_freq_will                float64\n",
      "word_freq_people              float64\n",
      "word_freq_report              float64\n",
      "word_freq_addresses           float64\n",
      "word_freq_free                float64\n",
      "word_freq_business            float64\n",
      "word_freq_email               float64\n",
      "word_freq_you                 float64\n",
      "word_freq_credit              float64\n",
      "word_freq_your                float64\n",
      "word_freq_font                float64\n",
      "word_freq_000                 float64\n",
      "word_freq_money               float64\n",
      "word_freq_hp                  float64\n",
      "word_freq_hpl                 float64\n",
      "word_freq_george              float64\n",
      "word_freq_650                 float64\n",
      "word_freq_lab                 float64\n",
      "word_freq_labs                float64\n",
      "word_freq_telnet              float64\n",
      "word_freq_857                 float64\n",
      "word_freq_data                float64\n",
      "word_freq_415                 float64\n",
      "word_freq_85                  float64\n",
      "word_freq_technology          float64\n",
      "word_freq_1999                float64\n",
      "word_freq_parts               float64\n",
      "word_freq_pm                  float64\n",
      "word_freq_direct              float64\n",
      "word_freq_cs                  float64\n",
      "word_freq_meeting             float64\n",
      "word_freq_original            float64\n",
      "word_freq_project             float64\n",
      "word_freq_re                  float64\n",
      "word_freq_edu                 float64\n",
      "word_freq_table               float64\n",
      "word_freq_conference          float64\n",
      "char_freq_;                   float64\n",
      "char_freq_(                   float64\n",
      "char_freq_[                   float64\n",
      "char_freq_!                   float64\n",
      "char_freq_$                   float64\n",
      "char_freq_#                   float64\n",
      "capital_run_length_average    float64\n",
      "capital_run_length_longest      int64\n",
      "capital_run_length_total        int64\n",
      "dtype: object\n",
      "\n",
      "Type of X_small: \n",
      "object\n",
      "\n",
      "Type of y_small: \n",
      "int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: ADD YOUR CODE HERE\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Use train_test_split to prepare X_small and y_small with onyl 1% of the rows\n",
    "X_big, X_small, y_big, y_small = train_test_split(X, y, test_size=0.01, random_state=174)\n",
    "\n",
    "print(f\"Shape of X_small: {X_small.shape}\")\n",
    "print(f\"Shape of y_small: {y_small.shape}\\n\")\n",
    "print(f\"Type of X_small Columns: \\n{X_small.dtypes}\\n\")\n",
    "print(f\"Type of X_small: \\n{X_small.dtypes.dtype}\\n\")\n",
    "print(f\"Type of y_small: \\n{y_small.dtypes}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Train and evaluate models\n",
    "\n",
    "1. Import `LogisticRegression` from sklearn\n",
    "2. Instantiate model `LogisticRegression(max_iter=2000)`.\n",
    "3. Create a pandas DataFrame `results` with columns: Data size, training accuracy, validation accuracy\n",
    "4. Call your convenience function `get_classifier_accuracy()` using \n",
    "    - `X` and `y`\n",
    "    - Only first two columns of `X` and `y`\n",
    "    - `X_small` and `y_small`\n",
    "5. Add the data size, training and validation accuracy for each call to the `results` DataFrame\n",
    "6. Print `results`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data Size</th>\n",
       "      <th>Training Accuracy</th>\n",
       "      <th>Validation Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(4600, 57)</td>\n",
       "      <td>0.934783</td>\n",
       "      <td>0.918261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(4600, 2)</td>\n",
       "      <td>0.608986</td>\n",
       "      <td>0.613043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(46, 57)</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Data Size  Training Accuracy  Validation Accuracy\n",
       "0  (4600, 57)           0.934783             0.918261\n",
       "1   (4600, 2)           0.608986             0.613043\n",
       "2    (46, 57)           1.000000             0.833333"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: ADD YOUR CODE HERE\n",
    "# Import LogisticRegression from sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Instantiate model: Set model to a model object before you can perform ML on data \n",
    "model = LogisticRegression(max_iter=2000)\n",
    "\n",
    "# Create results DataFrame\n",
    "results = pd.DataFrame(columns=['Data Size', 'Training Accuracy', 'Validation Accuracy'])\n",
    "\n",
    "# Restrict X and y to two columns\n",
    "X_first_two_cols = X.iloc[:, 0:2]\n",
    "y_first_two_cols = y.iloc[:]\n",
    "\n",
    "# Obtain training and validation accuracy scores with all data\n",
    "first_result = get_classifier_accuracy(model, X, y)\n",
    "# Obtain training and validation accuracy scores with only the first two features\n",
    "second_result = get_classifier_accuracy(model, X_first_two_cols, y_first_two_cols)\n",
    "# Obtain training and validation accuracy scores with only 1% of the rows used\n",
    "third_result = get_classifier_accuracy(model, X_small, y_small)\n",
    "\n",
    "# For each result, assign data_size based on the data used for that result\n",
    "# Then add new row with data_size, training accuracy, and validation accuracy to the results DataFrame \n",
    "for result in [first_result, second_result, third_result]:\n",
    "    data_size = None\n",
    "    if(result == first_result):\n",
    "        data_size = X.shape\n",
    "    elif(result == second_result):\n",
    "        data_size = X_first_two_cols.shape\n",
    "    elif(result == third_result):\n",
    "        data_size = X_small.shape\n",
    "        \n",
    "    new_row = {'Data Size': data_size, 'Training Accuracy': result[0], 'Validation Accuracy':result[1]}\n",
    "    results = results.append(new_row, ignore_index=True)\n",
    "\n",
    "# Display results DataFrame\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Questions\n",
    "1. What is the validation accuracy using all data? What is the difference between training and validation accuracy?\n",
    "1. How does the validation accuracy and difference between training and validation change when only two columns are used? Provide values.\n",
    "1. How does the validation accuracy and difference between training and validation change when only 1% of the rows are used? Provide values.\n",
    "\n",
    "Answer for 1: When using all data samples and all features, the validation accuracy is 0.918261. This is only 0.016522 (or 1.6522%) less than the training accuracy of 0.934783.\n",
    "\n",
    "Answer for 2: When using all data samples but limiting the features to the first two columns, the validation accuracy is 0.613043. This is only 0.004057 (or 0.4057%) less than the training accuracy of 0.608986, which is slightly less than the difference between the training and validation accuracies when all samples and features are used. Restricting the features to the two first columns only had a minimal effect on the difference between training and validation accuracy, but the performance of the model was signifcantly worse as both the training and validation accuracy scores dropped by over 30%, compared to using all data samples and all features.\n",
    "\n",
    "Answer for 3: When only using 1% of the data rows with all features included, the validation accuracy is 0.833333. This is 0.166667(16.7%) less than the training accuracy of 1. The larger discrepencies in accuraacy can be attributed to the smaller size of our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Regression\n",
    "\n",
    "Using yellowbrick energy - regression  \n",
    "https://www.scikit-yb.org/en/latest/api/datasets/energy.html\n",
    "\n",
    "The goal is to investigate `LinearRegression()` and effects of reducing the number of features and number of samples on regression performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Implement convenience function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def get_regressor_mse(model, X, y):\n",
    "    '''Calculate train and validation mean-squared error (mse) of regressor (model)\n",
    "        \n",
    "        Splits feature matrix X and target vector y \n",
    "        with sklearn train_test_split() and random_state=956.\n",
    "        \n",
    "        model (sklearn regressor): Regressor to train and evaluate\n",
    "        X (numpy.array or pandas.DataFrame): Feature matrix\n",
    "        y (numpy.array or pandas.Series): Target vector\n",
    "        \n",
    "        returns: training mse, validation mse\n",
    "    \n",
    "    '''\n",
    "   \n",
    "    #TODO: IMPLEMENT FUNCTION BODY\n",
    "    # Split arrays or matrices into random train and test subsets. Use default 75/25 split?\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=956)\n",
    "    \n",
    "    # Fit the model according to the given training vector X_train and its relative target vector y_train\n",
    "    # Predict class labels for samples in X_test - X_test is the data matrix for which we want to get predictions\n",
    "    # Returns vector containing the class labels for each sample in X_test\n",
    "    model.fit(X_train, y_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Predict class labels for samples in X_train - X_train is the data matrix for which we want to get predictions\n",
    "    # Returns vector containing the class labels for each sample in X_train\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    \n",
    "    # Use the true labels and predicted labels(as returned by classifier) for the validation(testing) set and \n",
    "    # training set to determine mean squared error\n",
    "    validation_mse = mean_squared_error(y_test, y_test_pred) # Fraction of correctly classified samples\n",
    "    training_mse = mean_squared_error(y_train, y_train_pred)\n",
    "    \n",
    "    return((training_mse, validation_mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Load data\n",
    "\n",
    "Use the yellowbrick function `load_energy()` load the energy data set into feature matrix `X` and target vector `y`.\n",
    "\n",
    "Print dimensions and type of `X` and `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (768, 8)\n",
      "Shape of y: (768,)\n",
      "\n",
      "Type of X columns: \n",
      "relative compactness         float64\n",
      "surface area                 float64\n",
      "wall area                    float64\n",
      "roof area                    float64\n",
      "overall height               float64\n",
      "orientation                    int64\n",
      "glazing area                 float64\n",
      "glazing area distribution      int64\n",
      "dtype: object\n",
      "\n",
      "Type of X: \n",
      "object\n",
      "\n",
      "Type of y: \n",
      "float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: ADD YOUR CODE HERE\n",
    "from yellowbrick.datasets.loaders import load_energy\n",
    "\n",
    "# Load spam data set into feature matrix X and target vector y\n",
    "X, y = load_energy()\n",
    "# X.head()\n",
    "# y.head()\n",
    "print(f\"Shape of X: {X.shape}\")\n",
    "print(f\"Shape of y: {y.shape}\\n\")\n",
    "print(f\"Type of X columns: \\n{X.dtypes}\\n\")\n",
    "print(f\"Type of X: \\n{X.dtypes.dtype}\\n\")\n",
    "print(f\"Type of y: \\n{y.dtypes}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the sklearn function `train_test_split()` prepare a feature matrix `X_small` and target vector `y_small` that contain only **1%** of the rows. Use `random_state=174`.\n",
    "\n",
    "Print size and type of `X_small` and `y_small`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_small: (8, 8)\n",
      "Shape of y_small: (8,)\n",
      "\n",
      "Type of X_small columns: \n",
      "relative compactness         float64\n",
      "surface area                 float64\n",
      "wall area                    float64\n",
      "roof area                    float64\n",
      "overall height               float64\n",
      "orientation                    int64\n",
      "glazing area                 float64\n",
      "glazing area distribution      int64\n",
      "dtype: object\n",
      "\n",
      "Type of X_small: \n",
      "object\n",
      "\n",
      "Type of y_small: \n",
      "float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: ADD YOUR CODE HERE\n",
    "# Split data into training and testing sets before fitting model to data\n",
    "# Question - Is X_small and y_small supposed to be the test or training set?\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_big, X_small, y_big, y_small = train_test_split(X, y, test_size=0.01, random_state=174)\n",
    "\n",
    "print(f\"Shape of X_small: {X_small.shape}\")\n",
    "print(f\"Shape of y_small: {y_small.shape}\\n\")\n",
    "print(f\"Type of X_small columns: \\n{X_small.dtypes}\\n\")\n",
    "print(f\"Type of X_small: \\n{X_small.dtypes.dtype}\\n\")\n",
    "print(f\"Type of y_small: \\n{y_small.dtypes}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Train and evaluate models\n",
    "\n",
    "1. Import `LinearRegression` from sklearn\n",
    "2. Instantiate model `LinearRegression()`.\n",
    "3. Create a pandas DataFrame `results` with columns: Data size, training MSE, validation MSE\n",
    "4. Call your convenience function `get_regressor_mse()` using \n",
    "    - `X` and `y`\n",
    "    - Only first two columns of `X` and `y`\n",
    "    - `X_small` and `y_small`\n",
    "5. Add the data size, training and validation MSE for each call to the `results` DataFrame\n",
    "6. Print `results`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data Size</th>\n",
       "      <th>Training MSE</th>\n",
       "      <th>Validation MSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(768, 8)</td>\n",
       "      <td>7.972066e+00</td>\n",
       "      <td>10.318507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(768, 2)</td>\n",
       "      <td>5.360043e+01</td>\n",
       "      <td>46.410426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(8, 8)</td>\n",
       "      <td>8.126845e-27</td>\n",
       "      <td>489.163464</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Data Size  Training MSE  Validation MSE\n",
       "0  (768, 8)  7.972066e+00       10.318507\n",
       "1  (768, 2)  5.360043e+01       46.410426\n",
       "2    (8, 8)  8.126845e-27      489.163464"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: ADD YOUR CODE HERE\n",
    "# Import LinearRegression from sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Instantiate model: Set model to a model object before you can perform ML on data \n",
    "# fit_intercept=False sets the y-intercept to 0. \n",
    "# If fit_intercept=True, the y-intercept will be determined by the line of best fit.\n",
    "model = LinearRegression(fit_intercept=True)\n",
    "\n",
    "# Create results DataFrame\n",
    "results = pd.DataFrame(columns=['Data Size', 'Training MSE', 'Validation MSE'])\n",
    "\n",
    "# Restrict X and y to two columns\n",
    "X_first_two_cols = X.iloc[:, 0:2]\n",
    "y_first_two_cols = y.iloc[:]\n",
    "\n",
    "# Obtain training and validation MSE with all data\n",
    "first_result = get_regressor_mse(model, X, y)\n",
    "# Obtain training and validation MSE with only the first two features\n",
    "second_result = get_regressor_mse(model, X_first_two_cols, y_first_two_cols)\n",
    "# Obtain training and validation MSE with only 1% of the rows used\n",
    "third_result = get_regressor_mse(model, X_small, y_small)\n",
    "\n",
    "# For each result, assign data_size based on the data used for that result\n",
    "# Then add new row with data_size, training MSE, and validation MSE to the results DataFrame \n",
    "for result in [first_result, second_result, third_result]:\n",
    "    data_size = None\n",
    "    if(result == first_result):\n",
    "        data_size = X.shape\n",
    "    elif(result == second_result):\n",
    "        data_size = X_first_two_cols.shape\n",
    "    elif(result == third_result):\n",
    "        data_size = X_small.shape\n",
    "        \n",
    "    new_row = {'Data Size': data_size, 'Training MSE': result[0], 'Validation MSE':result[1]}\n",
    "    results = results.append(new_row, ignore_index=True)\n",
    "\n",
    "# Display results DataFrame\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Questions\n",
    "1. What is the validation MSE using all data? What is the difference between training and validation MSE?\n",
    "1. How does the validation MSE and difference between training and validation change when only two columns are used? Provide values.\n",
    "1. How does the validation MSE and difference between training and validation change when only 1% of the rows are used? Provide values.\n",
    "\n",
    "Answer for 1: When using all data samples and all features, the validation mean squared error is 10.318507. This is 2.346441 more than the training mean squared error of 7.972066.\n",
    "\n",
    "Answer for 2: When using all data samples but limiting the features to the first two columns, the validation mean squared error is 46.410426. This is 7.190004 less than the training mean squared error accuracy of 53.60043. Restricting the features to the two first columns increased the difference between training and validation MSE, and the performance of the model was signifcantly worse as both the training and validation mean squared errors increased compared to using all data samples and all features.\n",
    "\n",
    "Answer for 3: When only using 1% of the data rows with all features included, the validation mean squared error is 489.163464. This is exponentially larger than the training mean squared error of 8.126845e-27, and results in a massive difference between the two MSE values. The seemingly extreme values and the difference between them can be attributed to the small size of our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Observations/Interpretation\n",
    "\n",
    "Describe any pattern you see in the results. Relate your findings to what we discussed during lectures. Include data to justify your findings.\n",
    "\n",
    "\n",
    "Answer: With the exception of the accuracy score and MSE obtained for the dataset limited to two features, the results came out as expected, with the Validation MSE and accuracy scores being worse than the training MSE and accuracy scores. This is expected because the model will be a better fit to the data it has already has seen, compared to the data it has not yet seen. When using all data samples and all features, the accuracy scores (0.934783 and 0.918261) are at their highest values and the mean squared errors (7.972066 and 10.318507) are at their lowest compared to other data sets. When only using 1% of the data samples or using all of the data samples with only two features, the accuracy scores become lower and the MSEs become higher - this makes sense as the data set is less descriptive and has a smaller sample size.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Reflection\n",
    "Include a sentence or two about:\n",
    "- what you liked or disliked,\n",
    "- found interesting, confusing, challangeing, motivating\n",
    "while working on this assignment.\n",
    "\n",
    "\n",
    "Answer: I liked that this lab gave us the opportunity to apply the 5 steps of the machine learning workflow that we learned in class. I thought it was interesting how the train_test_split function could be used to split the dataset into different sizes. This assignment provided excellent practice for using the Scikit-learn library to train a model and use it to predict labels for new data. Learning how to properly load data with the Yellowbricks library was probably the most challenging part of this assignment. It was also interesting that cutting the number of features to two in the dataset resulted in a model that yielded better accuracy and a smaller MSE for the validation set (61.3% accuracy, 46.4 MSE) compared to the training set (60.9% accuracy, 53.6 MSE).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
